{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Dict, List, Optional\nfrom collections import Counter\nimport os\nimport csv\n#!pip install torchmetrics\n#!pip install pytorch-metric-learning\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n#!pip install pytorch-lightning\nimport torch.optim as optim\nimport torchmetrics\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer","metadata":{"_uuid":"b258c1bc-e67f-421d-a802-ff17479d31a5","_cell_guid":"bfe0d716-0047-47ce-9211-d4e7467b5229","collapsed":false,"id":"SWWjBbIZ5LSi","execution":{"iopub.status.busy":"2023-03-17T01:46:09.206729Z","iopub.execute_input":"2023-03-17T01:46:09.208214Z","iopub.status.idle":"2023-03-17T01:46:09.216466Z","shell.execute_reply.started":"2023-03-17T01:46:09.208094Z","shell.execute_reply":"2023-03-17T01:46:09.215097Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`encode`: encode a given space-separated text into list of token ids according to the `self.token2idx` property. For tokens not present in the mapping, use the id of the `<unk>` token. If `max_length` is set, pad the input to `max_length` if it is less than `max_length` and truncate to `max_length` if it exceeds the length.\n\nExamples\n```python\ntext = \"hello transformers !\"\ntokenizer.encode(text)                  # example output: [3, 4, 5]\ntokenizer.encode(text, max_length=5)    # example output: [3, 4, 5, 0, 0]\ntokenizer.encode(text, max_length=2)    # example output: [3, 4]\n```","metadata":{"_uuid":"9fc4566a-11f3-43f8-bb36-3e57ef2e20df","_cell_guid":"b74ef66a-97bd-47a9-9cd6-7158648dc907","id":"FF4glogI5LSk","trusted":true}},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self):\n        # two special tokens for padding and unknown\n        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n        self.idx2token = [\"<pad>\", \"<unk>\"]\n        self.is_fit = False\n    \n    @property\n    def pad_id(self):\n        return self.token2idx[\"<pad>\"]\n    \n    def __len__(self):\n        return len(self.idx2token)\n    \n    def fit(self, train_texts: List[str]):\n        counter = Counter()\n        for text in train_texts:\n            counter.update(text.lower().split())\n        \n        # manually set a vocabulary size for the data set\n        vocab_size = 20000\n        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n        for (i, token) in enumerate(self.idx2token):\n            self.token2idx[token] = i\n            \n        self.is_fit = True\n                \n    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n        if not self.is_fit:\n            raise Exception(\"Please fit the tokenizer on the training tokens\")\n            \n        # TODO: implement the encode method, the method signature shouldn't be changed\n        #ids list stores the return value\n        ids = []\n        #split tokens by space\n        words = text.split()\n        #for each token, tokenize. if not tokenizable, tokenize to <unk>.\n        for word in words:\n            if word not in self.token2idx:\n                ids.append(self.token2idx[\"<unk>\"])\n            else:\n                ids.append(self.token2idx[word])\n        #if there is a max_length, pad/truncate\n        if (max_length != None):\n            #truncate\n            if len(ids) > max_length:\n                diff = len(ids) - max_length\n                ids = ids[:-diff]\n            #pad\n            elif len(ids) < max_length:\n                diff = max_length - len(ids)\n                for i in range(diff):\n                    ids.append(self.token2idx[\"<pad>\"])\n        #raise NotImplemented\n        return ids","metadata":{"_uuid":"f8d92ce4-97f7-482f-825e-fae34eee82c9","_cell_guid":"c723c028-b03a-4899-9ca3-af2b035a9112","collapsed":false,"id":"u29mNAdI5LSl","execution":{"iopub.status.busy":"2023-03-17T01:46:11.019878Z","iopub.execute_input":"2023-03-17T01:46:11.020294Z","iopub.status.idle":"2023-03-17T01:46:11.035888Z","shell.execute_reply.started":"2023-03-17T01:46:11.020259Z","shell.execute_reply":"2023-03-17T01:46:11.034273Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_raw_data(filepath: str, with_tags: bool = True):\n    data = {'text': []}\n    if with_tags:\n        data['tags'] = []\n        with open(filepath) as f:\n            reader = csv.reader(f)\n            for text, tags in reader:\n                data['text'].append(text)\n                data['tags'].append(tags)\n    else:\n        with open(filepath) as f:\n            for line in f:\n                data['text'].append(line.strip())\n    return data","metadata":{"_uuid":"cba35332-406d-46f4-b900-84b80b9fb176","_cell_guid":"67831409-8a22-457f-a0c5-7f496e0c4af0","collapsed":false,"id":"7lHbdxRn5LSm","execution":{"iopub.status.busy":"2023-03-17T01:46:13.384751Z","iopub.execute_input":"2023-03-17T01:46:13.385181Z","iopub.status.idle":"2023-03-17T01:46:13.394446Z","shell.execute_reply.started":"2023-03-17T01:46:13.385144Z","shell.execute_reply":"2023-03-17T01:46:13.392650Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\n#added data_dir to get files\ndata_dir = \"/kaggle/input/190i-mp2\"\ntrain_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\nval_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\ntest_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n# fit the tokenizer on the training tokens\ntokenizer.fit(train_raw['text'])","metadata":{"_uuid":"9fdd4cca-39ce-4a58-8335-eba0e89fa236","_cell_guid":"a36b1670-2c51-4da7-8f78-a0ef714e36fa","collapsed":false,"id":"dEuoJh1Q5LSn","execution":{"iopub.status.busy":"2023-03-17T01:46:15.072077Z","iopub.execute_input":"2023-03-17T01:46:15.072520Z","iopub.status.idle":"2023-03-17T01:46:15.242893Z","shell.execute_reply.started":"2023-03-17T01:46:15.072461Z","shell.execute_reply":"2023-03-17T01:46:15.241871Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#upload the dataset\n#for google colb, use this\n#from google.colab import files\n#uploaded = files.upload()","metadata":{"_uuid":"ea442848-461e-4625-a202-ee15c6bc7a88","_cell_guid":"1902e593-0693-4584-9ee0-782ce856cf46","collapsed":false,"id":"94u1vPV-lbXf","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntrain_raw = load_raw_data(os.path.join(\"train.csv\"))\nval_raw = load_raw_data(os.path.join(\"val.csv\"))\ntest_raw = load_raw_data(os.path.join(\"test_tokens.txt\"), with_tags=False)\n# fit the tokenizer on the training tokens\ntokenizer.fit(train_raw['text'])","metadata":{"_uuid":"157e7729-f09f-4531-a69d-613db015d55f","_cell_guid":"22ef5440-88d4-4eca-8c97-3957bbc9f0fb","collapsed":false,"id":"jertEpiNlqXM","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NERDataset: \n    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n  \n    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n        self.tokenizer = tokenizer\n        self.token_ids = []\n        self.tag_ids = []\n        self.with_tags = False\n        for text in raw_data['text']:\n            self.token_ids.append(tokenizer.encode(text, max_length=max_length))\n        if 'tags' in raw_data:\n            self.with_tags = True\n            for tags in raw_data['tags']:\n                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n    \n    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n        if max_length is None:\n            return tag_ids\n        # truncate the tags if longer than max_length\n        if len(tag_ids) > max_length:\n            return tag_ids[:max_length]\n        # pad with 0s if shorter than max_length\n        else:\n            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n        \n    def __len__(self):\n        return len(self.token_ids)\n    \n    def __getitem__(self, idx):\n        token_ids = torch.LongTensor(self.token_ids[idx])\n        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n        if self.with_tags:\n            # for training and validation\n            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n        else:\n            # for testing\n            return token_ids, mask","metadata":{"_uuid":"a676d030-6ce3-4593-aa35-aa57e5464168","_cell_guid":"b570880f-184f-42aa-b2e6-dc779d072b93","collapsed":false,"id":"KzUsGMealyZb","execution":{"iopub.status.busy":"2023-03-17T01:46:17.106438Z","iopub.execute_input":"2023-03-17T01:46:17.106886Z","iopub.status.idle":"2023-03-17T01:46:17.119896Z","shell.execute_reply.started":"2023-03-17T01:46:17.106850Z","shell.execute_reply":"2023-03-17T01:46:17.118812Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_data = NERDataset(train_raw, tokenizer)\nva_data = NERDataset(val_raw, tokenizer)\nte_data = NERDataset(test_raw, tokenizer)","metadata":{"_uuid":"afaad72e-95eb-438b-a19b-d2d7ee1e9c71","_cell_guid":"40510d21-1687-4f9d-9cf7-e069c16e2f4b","collapsed":false,"id":"0kMIKu-p5LSo","execution":{"iopub.status.busy":"2023-03-17T01:46:19.760260Z","iopub.execute_input":"2023-03-17T01:46:19.760721Z","iopub.status.idle":"2023-03-17T01:46:20.490269Z","shell.execute_reply.started":"2023-03-17T01:46:19.760677Z","shell.execute_reply":"2023-03-17T01:46:20.489109Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `nn.Embedding` layer to embed input token ids to the embedding space\n- `nn.TransformerEncoder` layer to perform transformer operations\n- `nn.Linear` layer as the output layer to map the output to the number of classes\n\nAs we will be using the cross-entropy loss, an `nn.Softmax` or `nn.LogSoftmax` layer is not needed.\n\n- `src`: a `torch.LongTensor` of shape (batch_size, max_length, vocab_size) representing the input text tokens.\n\n- `src_mask`: a `torch.BoolTensor` of shape (batch_size, max_length) indicating whether an input position is padded. This is needed to prevent the transformer model attending to padded tokens.","metadata":{"_uuid":"cbadb035-5174-410b-87fa-b373cc36eac1","_cell_guid":"58f403d7-ee8d-4a7b-92f8-80161222fb6f","id":"QVOHqRsD5LSo","trusted":true}},{"cell_type":"code","source":"# TODO: implement the Transformer model architecture and forward method\nclass TransformerModel(nn.Module):\n\n    def __init__(self, vocab_size: int, embed_size: int, num_heads: int, hidden_size: int,\n                 num_layers: int, dropout: float = 0.5):\n        super().__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = PositionalEncoding(embed_size, dropout)\n        encoder_layers = TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n        self.encoder = nn.Embedding(vocab_size, embed_size)\n        self.embed_size = embed_size\n        self.decoder = nn.Linear(embed_size, vocab_size)\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n        src_mask = torch.zeros(32, 32)\n        src = self.encoder(src) * math.sqrt(self.embed_size)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(10)\n        #output for this model should be 10 classes (as mentioned in the instructions)\n        return output\n\n\ndef generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"_uuid":"63fdc8c1-56a3-46af-b308-c4a51777163c","_cell_guid":"f0d4bf86-3711-46c9-8067-1096bede49b1","collapsed":false,"id":"Jvy2pBQL5LSo","execution":{"iopub.status.busy":"2023-03-17T01:46:22.365932Z","iopub.execute_input":"2023-03-17T01:46:22.366414Z","iopub.status.idle":"2023-03-17T01:46:22.387020Z","shell.execute_reply.started":"2023-03-17T01:46:22.366369Z","shell.execute_reply":"2023-03-17T01:46:22.385934Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, embed_size: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / embed_size))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)","metadata":{"_uuid":"bd94397a-beb2-44a0-b73e-1bc6302aa5f9","_cell_guid":"0838540c-8759-42b1-9d87-3fafad615632","collapsed":false,"execution":{"iopub.status.busy":"2023-03-17T01:46:24.163892Z","iopub.execute_input":"2023-03-17T01:46:24.164652Z","iopub.status.idle":"2023-03-17T01:46:24.173896Z","shell.execute_reply.started":"2023-03-17T01:46:24.164609Z","shell.execute_reply":"2023-03-17T01:46:24.172583Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(\n    model: nn.Module, \n    dataloader: DataLoader, \n    device: torch.device,\n):\n    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n            # output shape: (batch_size, max_length, num_classes)\n            logits = model(input_ids, input_mask)\n            # ignore padding index 0 when calculating loss\n            loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n                \n            loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n            is_active = torch.logical_not(input_mask)  # non-padding elements\n            # only consider non-padded tokens when calculating accuracy\n            acc_metric.update(logits[is_active], tags[is_active])\n    \n    print(f\"| Validate | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")","metadata":{"_uuid":"60b17a5c-2856-4e1e-9cc0-28ea0ccf1868","_cell_guid":"59254fdc-a7c0-4711-8bb8-175dd0b0050f","collapsed":false,"id":"Y5Eaibzu5LSp","execution":{"iopub.status.busy":"2023-03-17T01:46:25.832879Z","iopub.execute_input":"2023-03-17T01:46:25.833353Z","iopub.status.idle":"2023-03-17T01:46:25.844729Z","shell.execute_reply.started":"2023-03-17T01:46:25.833312Z","shell.execute_reply":"2023-03-17T01:46:25.843255Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    model: nn.Module, \n    dataloader: DataLoader, \n    optimizer: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n):\n    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n    model.train()\n    \n    # loop through all batches in the training\n    for batch in tqdm(dataloader):\n        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n        optimizer.zero_grad()\n        # output shape: (batch_size, max_length, num_classes)\n        logits = model(input_ids, input_mask)\n        # ignore padding index 0 when calculating loss\n        loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n        \n        loss.backward()\n        optimizer.step()\n        \n        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n        is_active = torch.logical_not(input_mask)  # non-padding elements\n        # only consider non-padded tokens when calculating accuracy\n        acc_metric.update(logits[is_active], tags[is_active])\n    \n    print(f\"| Epoch {epoch} | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")","metadata":{"_uuid":"e743f974-5ad4-4c72-9985-e7659bc419b1","_cell_guid":"c75dbc8a-86cf-45d7-8a7b-a4e9067ef3b2","collapsed":false,"id":"qQtTOXRA5LSp","execution":{"iopub.status.busy":"2023-03-17T01:46:27.670119Z","iopub.execute_input":"2023-03-17T01:46:27.670599Z","iopub.status.idle":"2023-03-17T01:46:27.682176Z","shell.execute_reply.started":"2023-03-17T01:46:27.670556Z","shell.execute_reply":"2023-03-17T01:46:27.680674Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# data loaders\ntrain_dataloader = DataLoader(tr_data, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(va_data, batch_size=32)\ntest_dataloader = DataLoader(te_data, batch_size=32)\n\n# move the model to device\n\"\"\"model = TransformerModel(vocab_size = len(tokenizer), \n    embed_size = 256, \n    num_heads = 4, \n    hidden_size = 256,\n    num_layers = 2,).to(device)\"\"\"\n#after completing the base model, experimented with standard BERT models for NER\n\noptimizer = optim.Adam(model.parameters())\n\n\"\"\"for epoch in range(5):\n    train(model, train_dataloader, optimizer, device, epoch)\nvalidate(model, val_dataloader, device)\"\"\"","metadata":{"_uuid":"adcaf3a3-78bf-433f-ace6-beb6a6f00071","_cell_guid":"b22fecf7-370a-452e-968d-ddd9aeba3e18","collapsed":false,"id":"5Be4ZCs15LSq","execution":{"iopub.status.busy":"2023-03-17T01:46:41.837182Z","iopub.execute_input":"2023-03-17T01:46:41.837681Z","iopub.status.idle":"2023-03-17T01:46:41.853284Z","shell.execute_reply.started":"2023-03-17T01:46:41.837639Z","shell.execute_reply":"2023-03-17T01:46:41.851668Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make predictions on the validation data and evaluate entity-level F1 scores using conlleval script.\n\n`predict`: taking inputs of a trained model, a dataloader, and a torch device, predict the tags for all tokens in the data set. The output should be a nested list of lists, each containing tag predictions for a single sentence.\n\n    Input texts in the dataloader (2 sentences):\n    EU rejects German call\n    Only France and Britain backed Fischler 's proposal .\n    \n    Example output:\n    [['B-ORG', 'O', 'B-MISC', 'O'], ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']]","metadata":{"_uuid":"d27bf3ce-452b-483a-89ec-903dc54f5275","_cell_guid":"c9b007ef-1695-4d1a-802d-ab242d469011","id":"uQV7JhRl5LSq","trusted":true}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\n#model tried:\n#model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n#greater f1 score but with larger processing times\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"Nadim Ladki\"\n\nner_results = nlp(example)\nres = []\n\nimport re\n\nttt = [\"SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\", \"Nadim Ladki\"]\nlines = [line.rstrip() for line in ttt]\nfor line in lines:\n    ind = [(ele.start(), ele.end() - 1) for ele in re.finditer(r'\\S+', line)]\n    ner_results = nlp(line)\n\n    tag = []\n    for ranges in ind:\n        assigned = False\n        for i in ner_results:\n            if i['start'] == ranges[0]:\n                tag.append(i['entity'])\n                assigned = True\n                continue\n        if assigned == False:\n            tag.append(\"O\")\n    print(tag)","metadata":{"_uuid":"98252aeb-7742-4b28-9f83-33c08a8adb04","_cell_guid":"8e1c0cd1-589c-4849-b14a-8ad808fe330f","collapsed":false,"execution":{"iopub.status.busy":"2023-03-17T01:46:46.093231Z","iopub.execute_input":"2023-03-17T01:46:46.093707Z","iopub.status.idle":"2023-03-17T01:46:48.569001Z","shell.execute_reply.started":"2023-03-17T01:46:46.093667Z","shell.execute_reply":"2023-03-17T01:46:48.567586Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndef predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n    model.eval()\n    preds = []\n    \"\"\"with torch.no_grad():\n        for batch in tqdm(dataloader):\n            for i in batch:\n                ner_results = nlp(example)\n                res = []\n                for i in range(len(ner_results)):\n                    res.append(ner_results[i]['entity'])\n                preds.append(res)\n                print(__getitem__(self, i))\n    return preds\"\"\"\n    lines = []\n    #with open(\"/kaggle/input/190i-mp2/test_tokens.txt\") as file:\n    #    lines = [line.rstrip() for line in file]\n    #    count = 0\n    \n    #VALIDATION MODEL\n    lines = []\n    if dataloader == val_dataloader:\n        data = pd.read_csv(\"/kaggle/input/190i-mp2/val.csv\", header=None)\n        lines = [line.rstrip() for line in data.iloc[:,0]]\n    \n    elif dataloader == test_dataloader:\n        #TESTING MODEL FOR PRODUCING SUBMISSION.TXT (COMMENT OUT IF NOT)\n        with open(\"/kaggle/input/190i-mp2/test_tokens.txt\") as file:\n            lines = [line.rstrip() for line in file]\n\n    for line in lines:\n        nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n        example = line\n\n        ner_results = nlp(line)\n        ind = [(ele.start(), ele.end() - 1) for ele in re.finditer(r'\\S+', example)]\n\n        tag = []\n        for ranges in ind:\n            assigned = False\n            for i in ner_results:\n                if i['start'] == ranges[0]:\n                    tag.append(i['entity'])\n                    assigned = True\n                    continue\n            if assigned == False:\n                tag.append(\"O\")\n        preds.append((tag))\n        #print(line)\n        #print(tag)\n    return preds","metadata":{"_uuid":"7473b3bb-f8ec-44d4-971a-682ba3c7e904","_cell_guid":"c795f4a0-a3ac-4f70-ba7d-e710373e001f","collapsed":false,"id":"2BeTuu4i5LSq","execution":{"iopub.status.busy":"2023-03-17T01:46:54.027248Z","iopub.execute_input":"2023-03-17T01:46:54.027660Z","iopub.status.idle":"2023-03-17T01:46:54.039477Z","shell.execute_reply.started":"2023-03-17T01:46:54.027624Z","shell.execute_reply":"2023-03-17T01:46:54.038182Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\nfrom conlleval import evaluate","metadata":{"_uuid":"a6b2d383-4f08-4633-a75c-de9fa15df5aa","_cell_guid":"3e773534-fa3e-40f8-9990-473a397ecd7a","collapsed":false,"id":"EzFjEe0c5LSq","execution":{"iopub.status.busy":"2023-03-17T01:28:46.868614Z","iopub.execute_input":"2023-03-17T01:28:46.869414Z","iopub.status.idle":"2023-03-17T01:28:48.548271Z","shell.execute_reply.started":"2023-03-17T01:28:46.869368Z","shell.execute_reply":"2023-03-17T01:28:48.546366Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the conlleval script to measure the entity-level f1\npred_tags = []\nfor tags in predict(model, val_dataloader, device):\n    pred_tags.extend(tags)\n    pred_tags.append('O')\n    \ntrue_tags = []\nfor tags in val_raw['tags']:\n    true_tags.extend(tags.strip().split())\n    true_tags.append('O')\n\nevaluate(true_tags, pred_tags)","metadata":{"_uuid":"a4cfb4b0-96e4-4ac2-948c-4a80f22540fa","_cell_guid":"bdd9c690-54e7-47a7-8c01-ae49dfe80f2a","collapsed":false,"id":"lVAnQYdD5LSr","execution":{"iopub.status.busy":"2023-03-17T01:30:27.432760Z","iopub.execute_input":"2023-03-17T01:30:27.433248Z","iopub.status.idle":"2023-03-17T01:34:44.418181Z","shell.execute_reply.started":"2023-03-17T01:30:27.433204Z","shell.execute_reply":"2023-03-17T01:34:44.416861Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Example output from the above codeblock. We will take the overall test F1 score (69.24 in this example)\n```\nprocessed 54612 tokens with 5942 phrases; found: 5554 phrases; correct: 3980.\naccuracy:  65.78%; (non-O)\naccuracy:  93.88%; precision:  71.66%; recall:  66.98%; FB1:  69.24\n              LOC: precision:  84.58%; recall:  77.03%; FB1:  80.63  1673\n             MISC: precision:  77.31%; recall:  71.69%; FB1:  74.40  855\n              ORG: precision:  58.71%; recall:  63.83%; FB1:  61.16  1458\n              PER: precision:  66.84%; recall:  56.89%; FB1:  61.47  1568\n(71.66006481814908, 66.98081454055873, 69.24147529575504)\n```\nIf the codeblock above errors out, check your implementation of the `predict` function. It should return a nested list of lists, each containing predicted tags in their IOB string forms.","metadata":{"_uuid":"51e52285-96b4-4151-9870-7352ddbc577a","_cell_guid":"461153e3-7d85-402b-932a-b701c8141ea9","id":"ztKqd9J15LSr","trusted":true}},{"cell_type":"markdown","source":"Make Predictions","metadata":{"_uuid":"18e152ec-00c8-47ac-b49b-5be5df8150eb","_cell_guid":"8d7efbcc-bc8b-4670-afa7-cd8e2d2dcf5c","id":"IaChyXkY5LSr","trusted":true}},{"cell_type":"code","source":"# make prediction on the test set and save to submission.txt\npreds = predict(model, test_dataloader, device)\nwith open(\"submission.txt\", \"w\") as f:\n    for tags in preds:\n        f.write(\" \".join(tags) + \"\\n\")","metadata":{"_uuid":"9d4d6188-f7fe-4ee7-a7b6-514b874b1e52","_cell_guid":"e1afca7b-4cda-4505-a78f-5447c3969bfe","collapsed":false,"id":"dVt102qy5LSs","execution":{"iopub.status.busy":"2023-03-17T01:35:43.455059Z","iopub.execute_input":"2023-03-17T01:35:43.456526Z","iopub.status.idle":"2023-03-17T01:40:04.443240Z","shell.execute_reply.started":"2023-03-17T01:35:43.456433Z","shell.execute_reply":"2023-03-17T01:40:04.441863Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"_uuid":"611312ce-a580-4995-9573-c82dd4148f41","_cell_guid":"9913e43b-c04d-44c8-8462-20a6b0f06597","collapsed":false,"execution":{"iopub.status.busy":"2022-02-17T08:27:24.950886Z","iopub.execute_input":"2022-02-17T08:27:24.951166Z","iopub.status.idle":"2022-02-17T08:27:24.957143Z","shell.execute_reply.started":"2022-02-17T08:27:24.951135Z","shell.execute_reply":"2022-02-17T08:27:24.956414Z"},"id":"jJOtOKyR5LSs","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"_uuid":"e79df324-5e02-4bcc-9dde-33bc661e455c","_cell_guid":"e9a7b5d8-9244-4469-87a4-59621bbdc646","collapsed":false,"execution":{"iopub.status.busy":"2022-02-17T08:27:30.109566Z","iopub.execute_input":"2022-02-17T08:27:30.109841Z","iopub.status.idle":"2022-02-17T08:27:30.796831Z","shell.execute_reply.started":"2022-02-17T08:27:30.109811Z","shell.execute_reply":"2022-02-17T08:27:30.795875Z"},"id":"5VquJfjq5LSs","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"301ffba2-3316-4f73-9756-a36a1ba6166b","_cell_guid":"ea3f9b49-0c46-4b97-9e9a-4b0dc0139dd8","collapsed":false,"id":"ioUQaKri5LSs","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}